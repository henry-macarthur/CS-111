NAME: Henry MacArthur
EMAIL: HenryMac16@gmail.com
ID: 705096169

------------------
Contents included with lab2a-705096169.tar.gz:
--
lab2_add.c
    this is a c program that supports multithreading(number of threads specified by user). Overall,
    the program just adds one to a counter, then subtracts one from it, and times the operation, and outputs 
    relevant data to stdout in CSV format. In addition to specifiying thread count, a user can specify number of
    iterations each thread runs, wether or not the thread should initially yeild, and what type of lock or mechanism
    can b e used to protect critical sections.
lab2_list.c
    Similar to lab2_add.c but this program works on a sorted linked list not a basic counter. Each running thread, 
    when gained access to the lock. Inserts n elements(n = iterations) into the list, looks them up, deletes them, and then 
    gives up its lock to the list. It also allows users to specify spin or mutex locks, num threads, num iterations, 
    and wether or not a thread should yield. times data and outputs relavent info to stdout in csv format
SortedList.h 
    this is a header file that gives the interface for our linked list functionality
SortedList.c
    provides the doubly circular sorted linked list implementation: lookup,  insert, delete, and length
lab2_list.csv  
    CSV file that contains all the output gathered from specified test cases for the list program in the Makefile
lab2_add.csv
    CSV file that contains all the output gathered from specified test cases for the add program in the Makefile
lab2_list.gp
    generates all the plots for linked list related data(lab2_list.csv)
lab2_add.gp
    generates all the plots for Part 1 related data(lab2_add.csv)

lab2_add-[1-4].png
    these are all the plots generated by running lab2_add.gp with lab2_add.csv being popluated by the given test commands
    1: Plots threads and iterations that run without failure
    2: Plots the cost of yielding. Graphs threads and their relative cost per operation with and without yields
    3: Plots operation cost vs number of iterations for a single thread. 
    4: Plots num threads and num iterations, running unprotected, mutexes, spins, and compare and swap
     that run without failure
    5:
lab2_list=[1-4].png
    these are all the plots generated by running lab2_list.gp with lab2_list.csv being popluated by the gi in part 2
    1: Plots average cost per operation v iterations of a unprotected single thread
    2: Plots unprotected threads and iterations that run without failure, used to gauge multithreaded performance that can succeed without 
    synchronization but uses different types of yields
    3: Plots protected iterations that run without failure using yields and various synchronization methods
    4: Plots average cost per operation of multithreaded performance using mutexes v spin locks

Makefile
    build: builds both lab2_list.c and lab2_add.c
    add: builds the add program
    list: builds the list program
    dist: runs tests, generates graphs, and creates the tarball
    clean: removes executables and tarball
    graphs: runs tests for both programs and generates repective graphs and .csv files
    add-graph: generates graphs for the add program
    list-graph: generates graphs for the list program
    tests: generates the tests and .csv files for both the add and list program
    test-list: generates the tests and .csv files for the list program
    test-add: generates the tests and .csv files for the add program

    
------------------
2.1.1:
    The reason why it takes many iterations for errors to appear is because of the occurence of context switches.
    If we give a thread a lot of iterations, the os has to perform more context switches for each running thread, as
    more iterations increases run time. Longer run time means that the thread is more likely to use up its alloted
    time slice, possibly more than once, and the more context switches are performed, the higher the likelyhood for
    race conditions to occur. 
    Significantly smaller number of iterations seldom fails as a thread might not have to perform a context switch
    as the time alloted for its time slice is enough to complete its # of iterations. Since the threads are not using 
    up their time slice, and need not to be queued again, there is little room for race conditions to occur durring
    context switches. 
2.1.2
    The yield runs are so much slower beacuse right before the thread runs the addition functio, it is forced to yield. This means
    that as soon as it starts to run, it yields, and a context switch occurs, creating overhead, which inturn 
    slows down the program. The time is going to perform the context switch of threads on the cpu, meaning it has to 
    save registers, load registers, push and pop data onto/off the stack, etc and switch out the running program for a new one.
    
    I do not think it is possible to get valid per operation time as that would mean we would need to get the 
    time it takes to perform a context switch which is indeterministic, not deterministic. Also, there is a lot
    of overhead involved with creating a thread that is not part of an operation which will throw our
    calculations off. Overall, it is hard to actually determine how long the thread runs doing the calculation as 
    context switches and other overheads throw of our timer.
    At best, I think we could get a rough estimate of the per operation timing by estimating and subtracting the 
    cost of creation overhead and context switch overhead.

2.1.3
    The reason why cost per operation decreases as we increase the number of operations is because it starts to 
    balance out the overhead of threading and context switching. Creating a thread has a lot of overhead, none of which is related
    to operation cost, so as we increase operations, we have the program run for longer, hence further negating
    the constant and initial overhead impact of creating a thread as well as time spent switching processes.
    In order to estimate the correct cost, we can keep
    taking more data until this curve starts to flatten out/become more stable. Once we reach this 
    functional asymptote, we can estimate what the correct cost per operation is as unwanted overhead 
    becomes less and less impactful on our calculations.

2.1.4
    operations perform similarly for low number of threads because the threads do not have
    to wait as long for locks. given a low number of threads, when a thread waits to enter 
    a critical section, regardless of whether it is a spin lock, try and swap, or mutex, it does 
    not have to wait for a large number of threads to acquire and give up the locked critical section
    before it gets the lock to the critical section. Basically the difference in overhead associated with each is miniscule
    as each process does not have to wait for many others, so overhead does not accumulate much. As we increase the
    number of threads, there is more overhead, as more context switches have to occur as we switch between process.
    Also, the cost of time spent waiting for each critical section increases as a thread has to wait for more threads to get the lock(higher contention) 
    before it gets access. 

2.2.1
        The cost per operation when using a mutex lock seems to scale much worse in part 1 than part 2. Increasing the number of
    threads for the add program seems to have a direct negative effect on the cost per operation when using mutexes. This is understandable
    as the cost for mutex is comparable to the cost of the add commands. On the otherhand, when using the list program with Increasing
    number of threads, it seem as if the cost per operation of using mutexes is minimally effected. It also seems as if the cost
    per operation of the add program is longer than that of the list function when using mutexes for boths.
        The reason for why locking seems to have a direct(linear) effect on cost per operation is that the overhead of locking 
    is comparable to the cost of adding/subtracting in part 1. So when we increase threads, there is a noticable increase in
    time as the overhead of locking is directly related to the number of operations we perform (i.e one lock per 1 add and subtract).
    On the other hand, part 2 seems to stay reletively stable(flat graph) because the cost of locking is negligible in comparison to the 
    cost of the list operations we are performing. When we acquire the lock on a list, we have to insert, lookup, and delete 
    all nodes which takes considerable longer than the constant time associated with locking. In addition, the thread keeps the lock 
    much longer(insert, lookup, remove) than a thread does in part 1, so there is less context switching and overhead associated with 
    constantly giving up and accquiring locks. 


2.2.2
    In both cases, spin locks tend to start out more efficiently than mutex locks. Meaning that they tend to work better
    on a smaller number of running threads. As the number of threads decreases, so does their performance, and they end up
    scaling much worse than mutexes. The reason for this is that a spin lock has the tendency to idle the cpu while a 
    thread waits for a resource. So as we increase threads, the current thread has to wait(idle) longer(because there is a longer
    queue of waiting processes for the same resource) before it can enter the critical section and resume execution. A mutex
    scales better as it does not force a thread to idle while waiting for a resource. The current thread waits(sleep) for the resource to 
    become available. Unlike a spin lock where the thread remains idleing, in this case the thread allows another thread to execute till
    it can get the resource. With low number of threads, added overhead of performing context switches(which is worse than the idling time at low threads) 
    is likely why it performs worse than 
    spin locks, but it scales better as this overhead remains constant with increased threads while the idle time on spin locks increases, negatively
    impacting its performance curve. When looking at the shapes of their curves, both seem to be logarithmic
    but the spin lock increases at a higher rate than that of the mutex lock. 


----------------

http://web.cs.ucla.edu/classes/fall09/cs111/scribe/9/index.html
- outside of the readings and recomended webpages, I found this site to be very helpful with 
learning how to implement locks and synchronization